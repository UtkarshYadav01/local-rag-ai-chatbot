{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476ca0d25b660383",
   "metadata": {},
   "source": [
    "# 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "id": "98da13669632505f",
   "metadata": {},
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredFileLoader, \\\n",
    "    JSONLoader, TextLoader, CSVLoader\n",
    "\n",
    "\n",
    "# 1.load data\n",
    "def load_all_documents(folder_path: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Load all documents (PDF, DOCX, JSON, TXT, CSV, etc.) from a folder into LangChain Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = []\n",
    "    supported_exts = {'.pdf', '.docx', '.json', '.txt', '.csv'}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[-1].lower()\n",
    "\n",
    "            try:\n",
    "                if ext == '.pdf':\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                elif ext == '.docx':\n",
    "                    loader = UnstructuredWordDocumentLoader(file_path)\n",
    "                elif ext == '.json':\n",
    "                    loader = JSONLoader(\n",
    "                        file_path,\n",
    "                        jq_schema=\".\",  # You can change this if JSON has a specific key\n",
    "                        text_content=True\n",
    "                    )\n",
    "                elif ext == '.txt':\n",
    "                    loader = TextLoader(file_path, encoding='utf-8')\n",
    "                elif ext == '.csv':\n",
    "                    loader = CSVLoader(file_path)\n",
    "                else:\n",
    "                    # fallback for other formats\n",
    "                    loader = UnstructuredFileLoader(file_path)\n",
    "\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {file_path}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(docs)} documents from {folder_path}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "#running the step\n",
    "documents = load_all_documents()\n",
    "print(f\"total {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    print(f\"doc : {doc}\\n================================END======================================\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4c0fef0ed0ecae9",
   "metadata": {},
   "source": [
    "# 2. split data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5b09185eaeb4ae4",
   "metadata": {},
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 2. split data\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"‚úÇÔ∏è Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "#running the step\n",
    "chunks = split_documents(documents)\n",
    "for chunk in chunks:\n",
    "    print(f\"doc : {chunk}\\n================================END======================================\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be8d8a0516384fe8",
   "metadata": {},
   "source": [
    "# 3. Generating Unique IDs for Each Chunk"
   ]
  },
  {
   "cell_type": "code",
   "id": "db7fb106da990014",
   "metadata": {},
   "source": [
    "# 3. Generating Unique IDs for Each Chunk\n",
    "def calculate_chunk_ids(chunks):\n",
    "    # This will create IDs like \"data/sample.docx:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    print(f\"üî¢ Assigned unique IDs to {len(chunks)} chunk(s) ‚úÖ\\n\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "#running the step\n",
    "for chunk in calculate_chunk_ids(chunks):\n",
    "    print(f\"chunk : {chunk.metadata['id']}\\n===============================END=======================================\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3baa8ecd0a29af00",
   "metadata": {},
   "source": [
    "# 4. embed data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "19b32e72b15d48ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:43:12.499525Z",
     "start_time": "2025-10-26T17:43:11.938990Z"
    }
   },
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# 4. embed data\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    print(f\"üöÄ Embedding model initialized: llama3\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#running the step\n",
    "print(get_embedding_function())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Embedding model initialized: llama3\n",
      "model='llama3' validate_model_on_init=False base_url=None client_kwargs={} async_client_kwargs={} sync_client_kwargs={} mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None keep_alive=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "2ee4903bd74e0e27",
   "metadata": {},
   "source": [
    "\n",
    "# 5. reset db(optional)"
   ]
  },
  {
   "cell_type": "code",
   "id": "85aac11e65606dad",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# 5. reset db(optional)\n",
    "def clear_database():\n",
    "    paths_to_clear = [\"chroma\", \"data\"]  # List of directories to clear\n",
    "\n",
    "    for path in paths_to_clear:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"Deleted: {path}\")\n",
    "        else:\n",
    "            print(f\"Path does not exist: {path}\")\n",
    "    print(\"üßπ Database cleared\")\n",
    "\n",
    "\n",
    "#running the step\n",
    "clear_database()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea6c8f8dbbfd7c3e",
   "metadata": {},
   "source": [
    "# 6. store in db"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a2fdd2a8217dc12",
   "metadata": {},
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# 6. store in db\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=\"chroma\", embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\n",
    "\n",
    "\n",
    "#running the step\n",
    "add_to_chroma(chunks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88bfa63e2a5378fc",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cec521cd4ebe7",
   "metadata": {},
   "source": [
    "# 7. ask\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b330ea7837910b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:43:27.436005Z",
     "start_time": "2025-10-26T17:43:18.462022Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 1. ask\n",
    "def query_rag(query_text: str):\n",
    "    # 2. Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=\"chroma\", embedding_function=embedding_function)\n",
    "\n",
    "    # 3. template var\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an expert AI assistant specialized in analyzing Pharmacy Benefits Management (PBM) RFP documents.\n",
    "\n",
    "    Your task is to answer questions *strictly and only* based on the information provided in the following context.\n",
    "    Do not use any outside knowledge, assumptions, or general information unless explicitly stated in the context.\n",
    "\n",
    "    If the answer is not clearly stated or cannot be derived from the context, respond with:\n",
    "    \"I cannot find this information in the provided context.\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Answer (concise, factual, and supported by the context):\n",
    "    \"\"\"\n",
    "\n",
    "    # 4. Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    # 5. generate complete prompt\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    # 6.invoke llm\n",
    "    model = OllamaLLM(model=\"llama3\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # 7. get the original source\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    # print(formatted_response)\n",
    "    return response_text\n",
    "\n",
    "\n",
    "#running the step\n",
    "query_rag(\"What is this doc ?\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Embedding model initialized: llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkar\\AppData\\Local\\Temp\\ipykernel_3872\\4203683461.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=\"chroma\", embedding_function=embedding_function)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the context, it appears that this document is a Pharmacy Benefits Management (PBM) Request for Proposal (RFP) document.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:20:27.826193Z",
     "start_time": "2025-10-26T17:20:26.588615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# assume get_embedding_function(), CHROMA_PATH, LLM_MODEL are defined elsewhere\n",
    "\n",
    "# Initialize global memory to persist across queries\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from a Chroma DB, and generate a response using an Ollama model\n",
    "    that includes conversation history memory.\n",
    "    \"\"\"\n",
    "    # 1. Prepare the vector DB\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # 2. Define RAG prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a helpful assistant. Use the provided context and the conversation history to answer.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history}\n",
    "\n",
    "    ---\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Retrieve relevant documents\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # 4. Create prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "    # 5. Initialize LLM\n",
    "    model = OllamaLLM(model=LLM_MODEL)\n",
    "\n",
    "    # 6. Create a conversation chain with memory\n",
    "    conversation = ConversationChain(\n",
    "        llm=model,\n",
    "        memory=memory,\n",
    "        prompt=prompt_template,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 7. Generate a response\n",
    "    response = conversation.predict(context=context_text, question=query_text)\n",
    "\n",
    "    # 8. Collect sources\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "\n",
    "    formatted_response = f\"Response: {response}\\nSources: {sources}\"\n",
    "    return formatted_response\n"
   ],
   "id": "96b44b7499937d76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkar\\AppData\\Local\\Temp\\ipykernel_3872\\1607686374.py:16: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "dc25af000173ef2b",
   "metadata": {},
   "source": [
    "# 8. unit test"
   ]
  },
  {
   "cell_type": "code",
   "id": "96bef8265bc3ab6b",
   "metadata": {},
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# 1. prompt template\n",
    "EVAL_PROMPT = \"\"\"\n",
    "Expected Response: {expected_response}\n",
    "Actual Response: {actual_response}\n",
    "---\n",
    "(Answer with 'true' or 'false') Does the actual response match the expected response?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test_monopoly_rules():\n",
    "    assert query_and_validate(\n",
    "        question=\"How much total money does a player start with in Monopoly? (Answer with the number only)\",\n",
    "        expected_response=\"$1500\",\n",
    "    )\n",
    "\n",
    "\n",
    "def test_ticket_to_ride_rules():\n",
    "    assert query_and_validate(\n",
    "        question=\"How many points does the longest continuous train get in Ticket to Ride? (Answer with the number only)\",\n",
    "        expected_response=\"10 points\",\n",
    "    )\n",
    "\n",
    "\n",
    "# 2. ask\n",
    "def query_and_validate(question: str, expected_response: str):\n",
    "    response_text = query_rag(question)\n",
    "    prompt = EVAL_PROMPT.format(\n",
    "        expected_response=expected_response, actual_response=response_text\n",
    "    )\n",
    "\n",
    "    # 3. invoke llm\n",
    "    model = Ollama(model=\"llama3\")\n",
    "    evaluation_results_str = model.invoke(prompt)\n",
    "\n",
    "    # 4. clean\n",
    "    evaluation_results_str_cleaned = evaluation_results_str.strip().lower()\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    # 5. check\n",
    "    if \"true\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Green if it is correct.\n",
    "        print(\"\\033[92m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return True\n",
    "    elif \"false\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Red if it is incorrect.\n",
    "        print(\"\\033[91m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid evaluation result. Cannot determine if 'true' or 'false'.\"\n",
    "        )\n",
    "\n",
    "#running the step\n",
    "# test_monopoly_rules()\n",
    "# test_ticket_to_ride_rules()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48c81eca38518de4",
   "metadata": {},
   "source": [
    "from legacy.rag_pipeline_history import run_pipeline, clear_database\n",
    "clear_database()\n",
    "run_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
