{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. load data",
   "id": "476ca0d25b660383"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "#\n",
    "# # 1.load data pdf\n",
    "# DATA_PATH = \"data\"\n",
    "#\n",
    "#\n",
    "# def load_documents():\n",
    "#     document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "#     return document_loader.load()\n",
    "#\n",
    "#\n",
    "# documents = load_documents()\n",
    "# print(f\"total {len(documents)} documents\")"
   ],
   "id": "a8b86058c354d6ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "\n",
    "# 1.load data\n",
    "def load_documents():\n",
    "    document_loader = UnstructuredWordDocumentLoader(\"data/sample-rfp.docx\")\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "#running the step\n",
    "documents = load_documents()\n",
    "print(f\"total {len(documents)} documents\")\n",
    "print(documents[0])"
   ],
   "id": "7a6e2032543fb55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. split data\n",
   "id": "c4c0fef0ed0ecae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 2. split data\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "#running the step\n",
    "chunks = split_documents(documents)\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "print('----------------------')\n",
    "print(chunks[0])"
   ],
   "id": "b5b09185eaeb4ae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Generating Unique IDs for Each Chunk",
   "id": "be8d8a0516384fe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Generating Unique IDs for Each Chunk\n",
    "def calculate_chunk_ids(chunks):\n",
    "    # This will create IDs like \"data/sample.docx:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "#running the step\n",
    "for chunk in calculate_chunk_ids(chunks):\n",
    "    print(f\"chunk : {chunk}\\n======================================================================\")\n"
   ],
   "id": "db7fb106da990014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. embed data\n",
   "id": "3baa8ecd0a29af00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# 4. embed data\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#running the step\n",
    "print(get_embedding_function())"
   ],
   "id": "19b32e72b15d48ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 5. reset db(optional)"
   ],
   "id": "2ee4903bd74e0e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# 5. reset db(optional)\n",
    "def clear_database():\n",
    "    if os.path.exists(\"chroma\"):\n",
    "        shutil.rmtree(\"chroma\")\n",
    "\n",
    "\n",
    "#running the step\n",
    "clear_database()"
   ],
   "id": "dc02fafba7657b8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. store in db",
   "id": "ea6c8f8dbbfd7c3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# 6. store in db\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=\"chroma\", embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    else:\n",
    "        print(\"âœ… No new documents to add\")\n",
    "\n",
    "\n",
    "#running the step\n",
    "add_to_chroma(chunks)"
   ],
   "id": "9a2fdd2a8217dc12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "----------",
   "id": "88bfa63e2a5378fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. ask\n",
   "id": "4d5cec521cd4ebe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 1. ask\n",
    "def query_rag(query_text: str):\n",
    "    # 2. Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=\"chroma\", embedding_function=embedding_function)\n",
    "\n",
    "    # 3. prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Answer the question based on the above context: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # 4. Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    # 5. generate complete prompt\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    # 6.invoke llm\n",
    "    model = Ollama(model=\"llama3\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # 7. get the original source\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text\n",
    "\n",
    "\n",
    "#running the step\n",
    "query_rag(\"What is this doc ?\")"
   ],
   "id": "b330ea7837910b70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. unit test",
   "id": "dc25af000173ef2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# 1. prompt template\n",
    "EVAL_PROMPT = \"\"\"\n",
    "Expected Response: {expected_response}\n",
    "Actual Response: {actual_response}\n",
    "---\n",
    "(Answer with 'true' or 'false') Does the actual response match the expected response?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test_monopoly_rules():\n",
    "    assert query_and_validate(\n",
    "        question=\"How much total money does a player start with in Monopoly? (Answer with the number only)\",\n",
    "        expected_response=\"$1500\",\n",
    "    )\n",
    "\n",
    "\n",
    "def test_ticket_to_ride_rules():\n",
    "    assert query_and_validate(\n",
    "        question=\"How many points does the longest continuous train get in Ticket to Ride? (Answer with the number only)\",\n",
    "        expected_response=\"10 points\",\n",
    "    )\n",
    "\n",
    "# 2. ask\n",
    "def query_and_validate(question: str, expected_response: str):\n",
    "    response_text = query_rag(question)\n",
    "    prompt = EVAL_PROMPT.format(\n",
    "        expected_response=expected_response, actual_response=response_text\n",
    "    )\n",
    "\n",
    "    # 3. invoke llm\n",
    "    model = Ollama(model=\"llama3\")\n",
    "    evaluation_results_str = model.invoke(prompt)\n",
    "\n",
    "    # 4. clean\n",
    "    evaluation_results_str_cleaned = evaluation_results_str.strip().lower()\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    # 5. check\n",
    "    if \"true\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Green if it is correct.\n",
    "        print(\"\\033[92m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return True\n",
    "    elif \"false\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Red if it is incorrect.\n",
    "        print(\"\\033[91m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid evaluation result. Cannot determine if 'true' or 'false'.\"\n",
    "        )\n"
   ],
   "id": "96bef8265bc3ab6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_monopoly_rules()\n",
    "test_ticket_to_ride_rules()"
   ],
   "id": "542bbf69d2e21b7f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
